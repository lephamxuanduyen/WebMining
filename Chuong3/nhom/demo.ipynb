{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mf:\\Web_Mining\\Chuong3\\nhom\\demo.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Web_Mining/Chuong3/nhom/demo.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Web_Mining/Chuong3/nhom/demo.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Dau vao la 2 cau van ban\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Web_Mining/Chuong3/nhom/demo.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m Data \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mThe quick brown fox jumps over the lazy dog and\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Web_Mining/Chuong3/nhom/demo.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNever jump over the lazy dog quickly\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Dau vao la 2 cau van ban\n",
    "Data = [\"The quick brown fox jumps over the lazy dog and\",\n",
    "        \"Never jump over the lazy dog quickly\"]\n",
    "\n",
    "# Xay dung vector BOW\n",
    "vect = CountVectorizer()\n",
    "X = vect.fit_transform(Data)\n",
    "\n",
    "# Xay dung tu dien\n",
    "dictionary=list(vect.get_feature_names_out())\n",
    "\n",
    "print(\"Words in dictionary: \", dictionary)\n",
    "print(\"Vector Bag-of-Word: \\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is', 'is a', 'a test', 'test sentence']\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    # Tach tu\n",
    "    words = text.split()\n",
    "\n",
    "    # Su dung ham zip de tach cac cap tu lien tiep trong danh sach cac tu\n",
    "    ngrams = zip(*[words[i:] for i in range(n)])\n",
    "\n",
    "    # Su dung list comprehension de tao n-grams\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "text = \"this is a test sentence\"\n",
    "print(generate_ngrams(text, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['this', 'is', 'a', 'test', 'sentence']\n",
      "2-gram:  ['this is', 'is a', 'a test', 'test sentence']\n",
      "3-gram:  ['this is a', 'is a test', 'a test sentence']\n",
      "4-gram:  ['this is a test', 'is a test sentence']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    " \n",
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    " \n",
    "data = 'this is a test sentence'\n",
    " \n",
    "print(\"1-gram: \", extract_ngrams(data, 1))\n",
    "print(\"2-gram: \", extract_ngrams(data, 2))\n",
    "print(\"3-gram: \", extract_ngrams(data, 3))\n",
    "print(\"4-gram: \", extract_ngrams(data, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary N-gram:\n",
      "{'the': 15, 'quick': 13, 'brown': 1, 'fox': 5, 'jumps': 7, 'over': 11, 'lazy': 9, 'dog': 3, 'and': 0, 'the quick': 17, 'quick brown': 14, 'brown fox': 2, 'fox jumps': 6, 'jumps over': 8, 'over the': 12, 'the lazy': 16, 'lazy dog': 10, 'dog and': 4}\n",
      "[1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 2 1 1]\n",
      "{'never': 6, 'jump': 2, 'over': 8, 'the': 11, 'lazy': 4, 'dog': 0, 'quickly': 10, 'never jump': 7, 'jump over': 3, 'over the': 9, 'the lazy': 12, 'lazy dog': 5, 'dog quickly': 1}\n",
      "[0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Khởi tạo đối tượng CountVectorizer\n",
    "# Tham số ngram_range=(1,2) cho biết chúng ta muốn sử dụng N-grams từ 1 đến 2.\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "# countVector cũng dùng trong Bag of word nhưng được nạp thêm ngram_range\n",
    "\n",
    "# Tạo ra một list chứa các câu cần biểu diễn\n",
    "Data = [\"The quick brown fox jumps over the lazy dog and\",\n",
    "        \"Never jump over the lazy dog quickly\"]\n",
    "\n",
    "# Biểu diễn vector đặc trưng cho các câu\n",
    "features = vectorizer.fit_transform(Data)\n",
    "\n",
    "# In ra các feature vector tương ứng với các câu\n",
    "# Mỗi hàng trong ma trận tương ứng với một câu và mỗi cột tương ứng với một N-gram trong tất cả các câu. Giá trị của mỗi phần tử trong ma trận là số lần xuất hiện của N-gram đó trong câu tương ứng.\n",
    "arr=features.toarray()\n",
    "\n",
    "print(\"Vocabulary N-gram:\")\n",
    "# print(sorted(vectorizer.fit([\"The quick brown fox jumps over the lazy dog and\"]).vocabulary_))\n",
    "print(vectorizer.fit([\"The quick brown fox jumps over the lazy dog and\"]).vocabulary_)\n",
    "print(arr[0])\n",
    "\n",
    "# print(sorted(vectorizer.fit([\"Never jump over the lazy dog quickly\"]).vocabulary_))\n",
    "print(vectorizer.fit([\"Never jump over the lazy dog quickly\"]).vocabulary_)\n",
    "print(arr[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
